{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch2. Linear Regression\n",
    "\n",
    "## Overview\n",
    "[TOC]\n",
    "- Simple linear regression\n",
    "- Evaluating the model\n",
    "- Multiple linear regression\n",
    "- Polynomial regression\n",
    "- Regularization\n",
    "- Applying linear regression\n",
    "- Fitting models with gradient descent\n",
    "- Summary\n",
    "\n",
    "## Simple linear regression\n",
    "\n",
    "Training Data \n",
    "- used to estimate the **parameters** of a model. \n",
    "- Past Observations of **explanatory variables** + **respose variables**\n",
    "\n",
    "Prediction (using model)\n",
    "- using explanatory variables (that have not been previously observes)\n",
    "- estimate response variable\n",
    "\n",
    "Goal in Regression Problem:\n",
    "- predict the value of a continuous response variable\n",
    "\n",
    "Simple Linear Regression\n",
    "- linear relationship betw. 1 response variable and 1 explanatory variable\n",
    "\n",
    "### Ex: Pizza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "X = [[6], [8], [10], [14],   [18]]\n",
    "y = [[7], [9], [13], [17.5], [18]]\n",
    "plt.figure()\n",
    "plt.title('Pizza price plotted against diameter')\n",
    "plt.xlabel('Diameter in inches')\n",
    "plt.ylabel('Price in dollars')\n",
    "plt.plot(X, y, 'k.')\n",
    "plt.axis([0, 25, 0, 25])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Linear Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 12\" pizza should cost: $13.68\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# Training data\n",
    "X = [[6], [8], [10], [14],   [18]]\n",
    "y = [[7], [9], [13], [17.5], [18]]\n",
    "# Create and fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "print 'A 12\" pizza should cost: $%.2f' % model.predict([12])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperplane\n",
    "- assumption: linear relationship exists betw. response var. and explanatory var.\n",
    "- this relationship is modeled as a linear surface, **hyperplane**\n",
    "- subspace that's 1 dim. less than the ambient space that contains it\n",
    "\n",
    "### Estimator\n",
    "- eg: **`sklearn.linear_model.LinearRegression`** class\n",
    "- predicts a value based on the *observed* data\n",
    "- important methods (all estimator implements)\n",
    "\n",
    "name | role\n",
    "-|-\n",
    "`fit()` | learns the parameters of a model \n",
    "`predict()` | predict the value of response variable, using learned parameters\n",
    "\n",
    "```\n",
    "    y = α + βx\n",
    "```\n",
    "\n",
    "name | desc.\n",
    "-|-\n",
    "y | predicted value of the response var. \n",
    "x | explanatory variable\n",
    "α,β | intercept term / coefficient - paramters of the model (learned by the learning algorithm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Diameter in inches'); plt.ylabel('Price in dollars');\n",
    "X2 = [[0], [10], [14], [25]]\n",
    "plt.plot(X2, model.predict(X2), color='blue', linewidth=2)\n",
    "plt.plot(X,y, 'k.')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares (or Linear Least Squares)\n",
    "- model that produces the best fitting model\n",
    "\n",
    "** model that fits the training data? **\n",
    "\n",
    "### Evaluating the fitness of a model with a cost function\n",
    "\n",
    "**what's criteria for best-fitting regression?**\n",
    "\n",
    "#### Cost Function (aka Loss Function)\n",
    "\n",
    "Used to define & measure the error of a model\n",
    "\n",
    "- **residual** (or training error, 잔차): differences betw. predicted values & observed values in the training set\n",
    "- **prediction error / test error**: diff. in the test set\n",
    "\n",
    "**Residual sum of squares** cost function\n",
    "\n",
    "<img src=\"http://ww2.tnstate.edu/ganter/BIO-311-Ch12-Eq5a.gif\" width=200></img>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual sum of squares: 1.75\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print \"Residual sum of squares: %.2f\" % np.mean((model.predict(X) - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving ordinary least squares for simple linear regression\n",
    "\n",
    "#### β: calculate **variance** & **covariance** of x\n",
    "\n",
    "<img src='https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcSgqtL6JZb3CgHdXki2I-JimIc3hv5JH9jwN7KYEpYUiqrdN4tUDQ' width=200></img>\n",
    "\n",
    "\n",
    "<img src='http://education.howthemarketworks.com/wp-content/uploads/2013/09/Covariance-Formula.jpg' width=250></img>\n",
    "\n",
    "```\n",
    "β = cov(x,y) / var(x)\n",
    "```\n",
    "#### α\n",
    "\n",
    "```\n",
    "α = mean(y) - β mean(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "### r-squared\n",
    "\n",
    "how well the observed values of the response variables are predicted by the model\n",
    "- proportion of the variance in the response variable that is explaned by the model\n",
    "\n",
    "<img src='https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcQzfE5dS4wf2D645dxy8qrgQfrXkzijU0oGUa0iosrGlqMel2_oTg' width=200></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.6620\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = [[6], [8], [10], [14], [18]]; y = [[7], [9], [13], [17.5], [18]]\n",
    "X_test = [[8],  [9],  [11], [16], [12]]; y_test = [[11], [8.5], [15], [18], [11]]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "print 'R-squared: %.4f' % model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regiression \n",
    "\n",
    "### Formal Definition\n",
    "\n",
    "> multiple linear regression uses a coefficient for **each** of an arbitrary number of **explanatory variables**\n",
    "\n",
    "<img src='http://www.saedsayad.com/images/MLR_1b.png' width=500></img>\n",
    "\n",
    "in matrix form: \n",
    "\n",
    "<img src='http://wiki.stat.ucla.edu/socr/uploads/math/3/0/f/30fe0ab6589c9992f21487bd912085e7.png' height=18></img>\n",
    "\n",
    "<img src='http://wiki.stat.ucla.edu/socr/uploads/math/8/9/7/89764b9d5fee46cedb3301257d28aa51.png' width=400></img>\n",
    "\n",
    "### estimation of β (using matrix notation)\n",
    "<img src='http://wiki.stat.ucla.edu/socr/uploads/math/2/d/7/2d74063fcebd6bbebe99ace856d0207b.png' height=18/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.1875    ]\n",
      " [ 1.01041667]\n",
      " [ 0.39583333]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import inv; from numpy import dot, transpose\n",
    "X = [[1, 6, 2], [1, 8, 1], [1, 10, 0], [1, 14, 2], [1, 18, 0]]; y = [[7],    [9],    [13],    [17.5],  [18]]\n",
    "print dot(inv(dot(transpose(X), X)), dot(transpose(X), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or, use least square function that `NumPy` provides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.1875    ]\n",
      " [ 1.01041667]\n",
      " [ 0.39583333]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import lstsq\n",
    "print lstsq(X, y)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict `y` (with second explanatory variable)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [ 10.0625], Target: [11]\n",
      "Predicted: [ 10.28125], Target: [8.5]\n",
      "Predicted: [ 13.09375], Target: [15]\n",
      "Predicted: [ 18.14583333], Target: [18]\n",
      "Predicted: [ 13.3125], Target: [11]\n",
      "R-squared: 0.77\n"
     ]
    }
   ],
   "source": [
    "X = [[6, 2], [8, 1], [10, 0], [14, 2], [18, 0]]; y = [[7],    [9],    [13],    [17.5],  [18]]\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "X_test = [[8, 2], [9, 0], [11, 2], [16, 2], [12, 0]]; y_test = [[11],   [8.5],  [15],    [18],    [11]]\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "for i, prediction in enumerate(predictions):\n",
    "\tprint 'Predicted: %s, Target: %s' % (prediction, y_test[i])\n",
    "print 'R-squared: %.2f' % model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- additional explanatory variable improved the **performance** of the model\n",
    "- multiple linear regression model performs **significantly better** than the simple LR model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression \n",
    "\n",
    "### Quadratic Regression\n",
    "\n",
    "cf. `PolynomialFeatures` transformer can be used to add polynomial features to a feature representation\n",
    "\n",
    "cf. **Feature**: tuple of explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6], [8], [10], [14], [18]]\n",
      "[[  1   6  36]\n",
      " [  1   8  64]\n",
      " [  1  10 100]\n",
      " [  1  14 196]\n",
      " [  1  18 324]]\n",
      "[[6], [8], [11], [16]]\n",
      "[[  1   6  36]\n",
      " [  1   8  64]\n",
      " [  1  11 121]\n",
      " [  1  16 256]]\n",
      "Simple LR R^2 0.809726797708\n",
      "Quadratic Regression R^2 0.867544365635\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X_train = [[6], [8], [10], [14], [18]]\n",
    "y_train = [[7],    [9],    [13],    [17.5],  [18]]\n",
    "X_test = [[6],  [8],   [11], [16]]\n",
    "y_test = [[8],   [12],  [15], [18]]\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "xx = np.linspace(0, 26, 100)\n",
    "yy = regressor.predict(xx.reshape(xx.shape[0], 1))\n",
    "plt.plot(xx, yy)\n",
    "\n",
    "featurizer = PolynomialFeatures(degree=2)\n",
    "X_train_quad = featurizer.fit_transform(X_train)\n",
    "X_test_quad = featurizer.transform(X_test)\n",
    "\n",
    "regressor_quad = LinearRegression()\n",
    "regressor_quad.fit(X_train_quad, y_train)\n",
    "xx_quad = featurizer.transform(xx.reshape(xx.shape[0], 1))\n",
    "\n",
    "plt.plot(xx, regressor_quad.predict(xx_quad), c='r', linestyle='--')\n",
    "plt.title('Pizza price regressed on diameter (linear, quadratic)')\n",
    "plt.xlabel('Diameter in inches')\n",
    "plt.ylabel('Price in dollars')\n",
    "plt.axis([0, 25, 0, 25])\n",
    "plt.grid(True)\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.show()\n",
    "\n",
    "print X_train\n",
    "print X_train_quad\n",
    "print X_test\n",
    "print X_test_quad\n",
    "\n",
    "print 'Simple LR R^2', regressor.score(X_test, y_test)\n",
    "print 'Quadratic Regression R^2', regressor_quad.score(X_test_quad, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "- instead of induce a general rule,\n",
    "- memorized the inputs and outputs from the training data\n",
    "- so **performs poorly** on **test data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Collection of techniques that can be used to **prevent overfitting**\n",
    "\n",
    "**Penalty** against **complexity**\n",
    "\n",
    "\n",
    "### Ridge Regression \n",
    "\n",
    "using L2 norm of the coefficients\n",
    "\n",
    "### Least Absolute Shrinkage and Selection Operator (LASSO)\n",
    "\n",
    "using L1 norm of the coefficients\n",
    "\n",
    "### Elastic Net Regularization\n",
    "\n",
    "linearly combines L1 and L2 penalties(norms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Models with Gradient Descent\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "an optimization algorithm that can be used to estimate the local minimum of a function\n",
    "\n",
    "**learning rate** : controls the size of the steps\n",
    "- too small: too long time for convergence\n",
    "- too large: overstepping -> could oscillate around the optimal values\n",
    "\n",
    "type | training data coverage | result\n",
    "-|-|-\n",
    "Batch GD | all | deterministic\n",
    "Stochastic GD (SGD) | one / iteration | random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training set:  379\n",
      "length of test set:  127\n",
      "Cross validation r-sqaured scores: [ 0.67522661  0.69081592  0.76359336  0.63759407  0.85035132]\n",
      "Average cross validation r-squared score: 0.723516256254\n",
      "Test set r-squared score 0.623785816774\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "data = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target)\n",
    "print 'length of training set: ', len(y_train)\n",
    "print 'length of test set: ', len(y_test) # train:test = 3:1\n",
    "# random.seed(len(y_train))\n",
    "X_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "y_train = y_scaler.fit_transform(y_train)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "y_test = y_scaler.transform(y_test)\n",
    "regressor = SGDRegressor(loss='squared_loss')\n",
    "scores = cross_val_score(regressor, X_train, y_train, cv=5)\n",
    "print 'Cross validation r-sqaured scores:', scores\n",
    "print 'Average cross validation r-squared score:', np.mean(scores)\n",
    "regressor.fit_transform(X_train, y_train)\n",
    "print 'Test set r-squared score', regressor.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### 3 Cases of LR\n",
    "- Simple LR\n",
    "- Multiple LR\n",
    "- Polynomial Regression\n",
    "\n",
    "### Generalized Linear Model\n",
    "- framework for modeling linear relationships\n",
    "\n",
    "### How to minimize cost function (solve the model, find parameters, ...)\n",
    "- solve analytically (using Linear Algebra)\n",
    "- use gradient descent \n",
    "\n",
    "### Next Chapter\n",
    "- how to create features for different types of explanatory variables\n",
    "- especially, categorical variables, texts, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
